{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307cd125",
   "metadata": {},
   "source": [
    "# Using Runners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb1fd2",
   "metadata": {},
   "source": [
    "In BentoML, Runner represents a unit of computation that can be executed on a remote Python worker and scales independently.\n",
    "\n",
    "Runner allows bentoml.Service to parallelize multiple instances of a bentoml.Runnable class, each on its own Python worker. When a BentoServer is launched, a group of runner worker processes will be created, and `run` method calls made from the `bentoml.Service` code will be scheduled among those runner workers.\n",
    "\n",
    "Runner also supports Adaptive Batching. For a bentoml.Runnable configured with batching, multiple run method invocations made from other processes can be dynamically grouped into one batch execution in real-time. This is especially beneficial for compute intensive workloads such as model inference, helps to bring better performance through vectorization or multi-threading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aafd66",
   "metadata": {},
   "source": [
    "## Pre-built Model Runners\n",
    "\n",
    "BentoML provides pre-built Runners implemented for each ML framework supported. These pre-built runners are carefully configured to work well with each specific ML framework. They handle working with GPU when GPU is available, set the number of threads and number of workers automatically, and convert the model signatures to corresponding Runnable methods.\n",
    "\n",
    "```python\n",
    "trained_model = train()\n",
    "\n",
    "bentoml.pytorch.save_model(\n",
    "    \"demo_mnist\",    # model name in the local model store\n",
    "    trained_model,   # model instance being saved\n",
    "    signatures={     # model signatures for runner inference\n",
    "        \"predict\": {\n",
    "            \"batchable\": True,\n",
    "            \"batch_dim\": 0,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "runner = bentoml.pytorch.get(\"demo_mnist:lateset\").to_runner()\n",
    "runner.init_local()\n",
    "runner.predict.run( MODEL_INPUT )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366fe72",
   "metadata": {},
   "source": [
    "## Custom Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2258ea",
   "metadata": {},
   "source": [
    "### Creating a Runnable\n",
    "\n",
    "Runner can be created from a bentoml.Runnable class. By implementing a `Runnable` class, users can create Runner instances that runs custom logic. \n",
    "\n",
    "```python\n",
    "import time\n",
    "import typing as t\n",
    "from typing import TYPE_CHECKING\n",
    "from statistics import mean\n",
    "\n",
    "import nltk\n",
    "from utils import exponential_buckets\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import bentoml\n",
    "from bentoml.io import JSON\n",
    "from bentoml.io import Text\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from bentoml._internal.runner.runner import RunnerMethod\n",
    "    \n",
    "    class RunnerImpl(bentoml.Runner):\n",
    "        is_positive: RunnerMethod\n",
    "\n",
    "inference_duration = bentoml.metrics.Histogram(\n",
    "    name=\"inference_duration\",\n",
    "    documentation='Duration of inference',\n",
    "    labelnames=['nltk_version', 'sentiment_cls'],\n",
    "    bukects=(0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75,\n",
    "             1.0, 2.5, 5.0, 7.5, 10.0, float(\"inf\")),\n",
    ")\n",
    "\n",
    "polarity_counter = bentoml.metrics.Counter(\n",
    "    name='polarity_total',\n",
    "    documentation='Count total number of analysis by polarity scores',\n",
    "    labelnames=['polarity']\n",
    ")\n",
    "\n",
    "class NLTKSentimentAnalysisRunnable(bentoml.Runnable):\n",
    "    SUPPORTED_RESOURCES = (\"cpu\",)\n",
    "    SUPPORTS_CPU_MULTI_THREADING = False\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    @bentoml.Runnable.method(batchable=False)\n",
    "    def is_positive(self, input_text: str) -> bool:\n",
    "        start = time.perf_counter()\n",
    "        scores = [self.sia.polarity_scores(sentence)[\"compound\"] for sentence \n",
    "                  in nltk.sent_tokenize(input_text)]\n",
    "        inference_duration.labels(\n",
    "            nltk_version=nltk.__version__, \n",
    "            sentiment_cls=self.sia.__class__.__name__\n",
    "        ).observe(time.perf_counter() - start)\n",
    "        \n",
    "        return mean(scores) > 0\n",
    "\n",
    "nltk_runner = t.cast(\n",
    "    \"RunnerImpl\", bentoml.Runner(NLTKSentimentAnalysisRunnable, name=\"nltk_sentiment\")\n",
    ")\n",
    "\n",
    "svc = bentoml.Service(\"sentiment_analyzer\", runners=[nltk_runner])\n",
    "\n",
    "@svc.api(input=Text(), output=JSON())\n",
    "async def analysis(input_text: str) -> dict[str, bool]:\n",
    "    is_positive = await nltk_runner.is_positive.async_run(input_text)\n",
    "    polarity_counter.labels(polarity=is_positive).inc()\n",
    "    return {\"is_positive\": is_positive}\n",
    "```\n",
    "\n",
    "The `bentoml.Runnable.method` decorator is used for creating `RunnableMethod`- the decorated method will be exposed as the runner interface for accessing remotely. RunnableMethod can be configured with a signature, which is defined same as the Model Signatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc3183",
   "metadata": {},
   "source": [
    "### Reusable Runnable\n",
    "Runnable class can also take `__init__` parameters to customize its behavior for different scenarios. The same Runnable class can also be used to create multiple runners and used in the same service\n",
    "\n",
    "```python\n",
    "import bentoml\n",
    "import torch\n",
    "\n",
    "class MyModelRunnable(bentoml.Runnable):\n",
    "    SUPPORTED_RESOURCES = (\"nvidia.com/gpu\",)\n",
    "    SUPPORTS_CPU_MULTI_THREADING = True\n",
    "    \n",
    "    def __init__(self, model_file):\n",
    "        self.model = torch.load_model(model_file)\n",
    "        \n",
    "    @bentoml.Runnable.method(batchable=True, batch_dim=0)\n",
    "    def predict(self, input_tensor):\n",
    "        return self.model(input_tensor)\n",
    "    \n",
    "my_runner_1 = bentoml.Runner(\n",
    "    MyModelRunnable,\n",
    "    name='mu_runner_1',\n",
    "    runnable_init_params={\n",
    "        \"model_file\": \"./saved_model_1.pt\",\n",
    "    }\n",
    ")\n",
    "\n",
    "my_runner_2 = bentoml.Runner(\n",
    "    MyModelRunnable,\n",
    "    name='mu_runner_2',\n",
    "    runnable_init_params={\n",
    "        \"model_file\": \"./saved_model_2.pt\",\n",
    "    }\n",
    ")\n",
    "\n",
    "svc = bentoml.Service(__name__, runners=[mu_runner_1, mu_runner_2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310cff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
