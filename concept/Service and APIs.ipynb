{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b63c9984",
   "metadata": {},
   "source": [
    "## Service and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e013432",
   "metadata": {},
   "source": [
    "### Creating a Service\n",
    "\n",
    "A BentoML service is composed of Runner s and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cf8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bentoml\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "# Creating Runner instance from a saved model\n",
    "iris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n",
    "\n",
    "# Create the iris_classifier_service with the ScikitLearn runner\n",
    "svc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def classify(input_series: np.ndarray) -> np.ndarray:\n",
    "    result = iris_clf_runner.predict.run(input_series)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082c009",
   "metadata": {},
   "source": [
    "Services are initialized through `bentoml.Service()` call, with the service name and a list of Runners required in the service:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9ce4ff",
   "metadata": {},
   "source": [
    "### Runners\n",
    "\n",
    "Runners represent a unit of serving logic that can be scaled horizontally to maximize throughput and resoure utilization\n",
    "\n",
    "```python\n",
    "runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc89b18",
   "metadata": {},
   "source": [
    "Runner created from a model will automatically choose the most optimal Runner configurations specific for the target ML framework.\n",
    "\n",
    "For example, if an ML framework releases the Python GIL and supports concurrent access natively, BentoML will create a single global instance of the runner worker and route all API requests to the global instance; otherwise, BentoML will create multiple instances of runners based on the available system resources. We also let advanced users to customize the runtime configurations to fine tune the runner performance. To learn more, please see the concepts/runner guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6574df",
   "metadata": {},
   "source": [
    "#### Debugging Runners\n",
    "\n",
    "Runners must be initialized in order to function. Normally, this is handled by BentoML internally when `bentoml serve` is called\n",
    "\n",
    "If you want to import and run a service without using BentoML, this must be done manually. For example, to debug a service called svc in service.py:\n",
    "\n",
    "```python\n",
    "from service import svc\n",
    "\n",
    "for runner in svc.runners:\n",
    "    runner.init_local()\n",
    "    \n",
    "result = svc.apis['my_endpoint'].func(inp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ca9ab",
   "metadata": {},
   "source": [
    "### Service APIs\n",
    "\n",
    "Inference APIs define how the service functionality can be called remotely. A service can have one or more APIs. An API consists of its input/output specs and a callback function:\n",
    "\n",
    "```python\n",
    "# Create new API and add it to \"svc\"\n",
    "@svc.api(input=NumpyNdarray(), ouput=NumpyNdarray())\n",
    "def predict(input_array: np.ndarray) -> np.ndarray:\n",
    "    # Define business logic\n",
    "    # Define pre-processing logic\n",
    "    result = runner.run(input_array)\n",
    "    \n",
    "    # Define post-processing logic\n",
    "    return result\n",
    "```\n",
    "\n",
    "By decorating a function with `@svc.api`, we declare that the function shall be invoked when this API is called. The API function is a great place for defining your serving logic, such as feature fetching, pre and post processing, and model inferences via Runners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f746348",
   "metadata": {},
   "source": [
    "#### Route\n",
    "By default, the function name becomes the endpoint URL. Users can also customize this URL via the `route` option\n",
    "\n",
    "```python\n",
    "@svc.api(\n",
    "    input=NumpyNdarray(),\n",
    "    output=NumpyNdarray(),\n",
    "    route=\"/v2/models/my_model/versions/v0/infer\",\n",
    ")\n",
    "def predict(input_array: np.ndarray) -> np.ndarray:\n",
    "    return runner.run(input_array)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e50aa23",
   "metadata": {},
   "source": [
    "#### Inference Context\n",
    "\n",
    "The context of an inference call can be accessed through the additional `bentoml.Context` argument added to the service API function. Both the request and response contexts can be accessed through the inference context for getting and setting the headers, cookies, and status codes.\n",
    "\n",
    "```python\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray(),)\n",
    "def predict(input_array: np.ndarray, ctx: bentoml.Context) -> np.ndarray:\n",
    "    # get request headers\n",
    "    request_headers = ctx.request.headers\n",
    "    \n",
    "    result = runner.run(input_array)\n",
    "    \n",
    "    # set response headers, cookies, and status code\n",
    "    ctx.response.status_code = 202\n",
    "    ctx.response.cookies = [\n",
    "        bentoml.Cookie(\n",
    "            key='key',\n",
    "            value='value',\n",
    "            max_age=None,\n",
    "            expires=None,\n",
    "            path='/predict',\n",
    "            domain=None,\n",
    "            secure=True,\n",
    "            httponly=True,\n",
    "            samesite=\"None\"\n",
    "        )\n",
    "    ]\n",
    "    ctx.response.headers.append(\"X-Custom-Header\", 'value')\n",
    "    \n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0c5bc",
   "metadata": {},
   "source": [
    "### IO Descriptors\n",
    "\n",
    "IO descriptors are used for defining an APIâ€™s input and output specifications. It describes the expected data type, helps validate that the input and output conform to the expected format and schema and convert them from and to the native types. They are specified through the input and output arguments in the @svc.api decorator method.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def classify(input_array: np.ndarray) -> np.ndarray:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Besides the NumpyNdarray IO descriptor, BentoML supports a variety of IO descriptors including `PandasDataFrame`, `JSON`, `String`, `Image`, `Text`, and `File`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be51697",
   "metadata": {},
   "source": [
    "#### Schema and Validation\n",
    "\n",
    "IO descriptors allow users to define the expected data types, shape, and schema, based on the type of the input and output descriptor specified. IO descriptors can also be defined through examples with the `from_sample` API to simplify the development of service definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1f997",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "\n",
    "The data type and shape of the `NumpyNdarray` can be specified with the `dtype` and `shape` arguments. By setting the `enforce_shape` and `enforce_dtype` arguments to `True`, the IO descriptor will strictly validate the input and output data based the specified data type and shape.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "svc = bentoml.Service(\"iris_clf:latest\")\n",
    "\n",
    "# Define IO descriptors through samples\n",
    "output_descriptor = NumpyNdarray.from_sample(np.array([[1.0, 2.0, 3.0, 4.0]]))\n",
    "\n",
    "@svc.api(\n",
    "    input=NumpyNdarray(\n",
    "        shape=(-1, 4),\n",
    "        dtype=np.float32,\n",
    "        enforce_dtype=True,\n",
    "        enforce_shape=True\n",
    "    ),\n",
    "    ouput=output_descriptor,\n",
    "def classofy(input_array: np.ndarray) -> np.ndarray:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbaeab",
   "metadata": {},
   "source": [
    "#### Pandas DataFrame\n",
    "\n",
    "The data type and shape of the `PandasDataFrame` can be specified with the `dtype` and `shape` arguments. By setting the `enforce_shape` and `enforce_dtype` arguments to True, the IO descriptor will strictly validate the input and output data based the specified data type and shape\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "from bentoml.io import PandasDataFrame\n",
    "\n",
    "svc = bentoml.Service(\"iris_classifier\")\n",
    "\n",
    "# Define IO descriptors through samples\n",
    "output_descriptor = PandasDataFrame.from_sample(pd.DataFrame([[5, 4, 3, 2]]))\n",
    "\n",
    "@svc.api(\n",
    "    input=PandasDataFrame(\n",
    "        orient='records',\n",
    "        dtype=np.float32,\n",
    "        enforce_dtype=True,\n",
    "        shape=(-1, 4),\n",
    "        enforce_shape=True\n",
    "    ),\n",
    "    output=output_descriptor,\n",
    ")\n",
    "def classify(input_series: pd.DataFrame) -> pd.DataFrame:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274d0aa",
   "metadata": {},
   "source": [
    "#### JSON\n",
    "\n",
    "The data type of a JSON IO descriptor can be specified through a `Pydantic model`. By setting a pydantic model, the IO descriptor will validate the input based on the specified pydantic model and return.\n",
    "\n",
    "```python\n",
    "from typing import Dict, Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "svc = bentoml.Service(\"iris_classifier\")\n",
    "\n",
    "class IrisFeatures(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "\n",
    "@svc.api(\n",
    "    input=JSON(pydantic_model=IrisFeatures),\n",
    "    ouput=JSON(),\n",
    ")\n",
    "def classify(input_series: IrisFeatures) -> Dict[str, Any]:\n",
    "    input_df = pd.DataFrame([input_data.dict()])\n",
    "    results = iris_clf_runner.predict.run(input_df).to_list()\n",
    "    return {\"predictions\": results}     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cf713",
   "metadata": {},
   "source": [
    "#### Built-in Types\n",
    "\n",
    "Beside `NumpyNdarray`, BentoML supports a variety of other built-in IO descriptor types under the bentoml.io module.\n",
    "\n",
    "|IO Descriptor|Type|Arguments|Schema Type|\n",
    "|---|---|---|---|\n",
    "|NumpyNdarray|numpy.ndarray|validate, schema|numpy.dtype|\n",
    "|PandasDataFrame|pandas.DataFrame|validate, schema|pandas.DataFrame.dtypes|\n",
    "|Json|Python native types|validate, schema|Pydantic.BaseModel|\n",
    "|Image|PIL.Image.Image|pilmodel, mime_type|   |\n",
    "|Text|str|   |   |\n",
    "|File|BytesIOFile|kind, mime_type|   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd39d2c",
   "metadata": {},
   "source": [
    "#### Composite Types\n",
    "The `Multipart` IO descriptors can be used to group multiple IO Descriptor instances, which allows the API function to accept multiple arguments or return multiple values.\n",
    "\n",
    "```python\n",
    "import typing as t\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from bentoml.io import NumpyNdarray, Json\n",
    "\n",
    "class FooModel(BaseModel):\n",
    "    field1: int\n",
    "    field2: float\n",
    "    field3: str\n",
    "\n",
    "my_np_input = NumpyNdarray.from_sample(np.ndarray(...))\n",
    "\n",
    "@svc.api(\n",
    "    input=Multipart(\n",
    "        arr=NumpyNdarray(schema=np.dtype(int, 4), validate=True),\n",
    "        json=Json(pydantic_model=FooModel),\n",
    "    )\n",
    "    output=NumpyNdarray(schema=np.dtype(int), validate=True),\n",
    ")\n",
    "def predict(arr: np.ndarray, json: t.Dict[str, t.Any]) -> np.ndarray:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80dfa76",
   "metadata": {},
   "source": [
    "### Sync vs Async APIs\n",
    "\n",
    "APIs can be defined as either synchronous function or asynchronous coroutines in Python. BentoML will intelligently create an optimally sized pool of workers to execute the synchronous logic. Synchronous APIs are simple and capable of getting the job done for most model serving scenarios.\n",
    "\n",
    "```python\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def predict(input_array: np.ndarray) -> np.ndarray:\n",
    "    result = runner.run(input_array)\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8813be5",
   "metadata": {},
   "source": [
    "Synchronous APIs fall short when we want to maximize the performance and throughput of the service. Asynchronous APIs are preferred if the processing logic is IO-bound or invokes multiple runners simultaneously.\n",
    "\n",
    "```python\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "# Load two runners for two different versions of the ScikitLearn\n",
    "runner1 = bentoml.sklearn.get('iris_clf:yftvuwkbbbi6zc').to_runner()\n",
    "runner2 = bentoml.sklearn.get('iris_clf:edq3adsfhzi6zg').to_runner()\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "async def predict(input_array: np.ndarray) -> np.ndarray:\n",
    "    # Call a remote feature store to pre-process the request\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get('https://features/get', params=input_array[0]) as resp:\n",
    "            features = get_features(await resp.text())\n",
    "    # Invoke both model runners simultaneously\n",
    "    results = await asyncio.gather(\n",
    "        runner1.predict.async_run(input_array, features),\n",
    "        runner2.predict.async_run(input_array, features),\n",
    "    )\n",
    "    \n",
    "    return combine_results(results)\n",
    "```\n",
    "\n",
    "The asynchronous API implementation is more efficient because when an asynchronous method is invoked, the event loop is released to service other requests while this request awaits the results of the method. In addition, BentoML will automatically configure the ideal amount of parallelism based on the available number of CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fe9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bentoml",
   "language": "python",
   "name": "bentoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
