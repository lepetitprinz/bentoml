{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea7b78f",
   "metadata": {},
   "source": [
    "## Preparing Models\n",
    "\n",
    "Save and version trained ML model with BentoML's model store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239b40c",
   "metadata": {},
   "source": [
    "### Save A Trained Model\n",
    "\n",
    "A trained ML model instance needs to be saved with BentoML API, in order to serve it with BentoML. For most cases, it will be just one line added to your model training pipeline, invoking a save_model call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec574bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: Model(tag=\"iris_clf:deocjxszyw3cllg6\")\n"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "# load training data set\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "# train the model\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(x, y)\n",
    "\n",
    "saved_model = bentoml.sklearn.save_model(\"iris_clf\", clf)\n",
    "print(f\"Model saved: {saved_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f61b1",
   "metadata": {},
   "source": [
    "We recommend **always save the model with BentoML as soon as it finished training and validation**. By putting the save_model call to the end of your training pipeline, all your finalized models can be managed in one place and ready for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9831b",
   "metadata": {},
   "source": [
    "Optionally, you may attach custom labels, metadata, or custom_objects to be saved alongside your model in the model store, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml.pytorch.save_model(\n",
    "    \"demo_mnist\",    # model name in the local model store\n",
    "    trained_model,   # model instance being saved\n",
    "    labels={         # user-defined labels for managing models in Yatai\n",
    "        \"owner\": \"nlp_team\",\n",
    "        \"stage\": \"dev\"\n",
    "    },\n",
    "    meetadata={    # user-defined additional metadata\n",
    "        \"acc\": acc,\n",
    "        \"cv_stats\": cv_stats,\n",
    "        \"dataset_version\": \"20220101\",\n",
    "    },\n",
    "    custom_object={    # save additional user-defined python object\n",
    "        \"tokenizer\": tokenizer_object,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb24e81",
   "metadata": {},
   "source": [
    "- labels: user-defined labels for managing models, e.g. team=nlp, stage=dev.\n",
    "\n",
    "- metadata: user-defined metadata for storing model training context information or model evaluation metrics, e.g. dataset version, training parameters, model scores.\n",
    "\n",
    "- custom_objects: user-defined additional python objects, e.g. a tokenizer instance, preprocessor function, model configuration json, serialized with cloudpickle. Custom objects will be serialized with cloudpickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac98332",
   "metadata": {},
   "source": [
    "### Retrieve a saved model\n",
    "\n",
    "To load the model instance back into memory, use the framework-specific `load_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47726d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bentoml\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "model : BaseEstimator = bentoml.sklearn.load_model(\"iris_clf:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50566d20",
   "metadata": {},
   "source": [
    "For retrieving the model metadata or custom objects, use the `get` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90a180e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_clf:deocjxszyw3cllg6\n",
      "/Users/yjkim/bentoml/models/iris_clf/deocjxszyw3cllg6\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "import bentoml\n",
    "bento_model: bentoml.Model = bentoml.models.get(\"iris_clf:latest\")\n",
    "\n",
    "print(bento_model.tag)\n",
    "print(bento_model.path)\n",
    "print(bento_model.custom_objects)\n",
    "print(bento_model.info.metadata)\n",
    "print(bento_model.info.labels)\n",
    "\n",
    "my_runner: bentoml.Runner = bento_model.to_runner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100fa2c",
   "metadata": {},
   "source": [
    "`bentoml.models.get` returns a bentoml.Model instance, which is a reference to a saved model entry in the BentoML model store. The bentoml.Model instance then provides access to the model info and the to_runner API for creating a Runner instance from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd2a70",
   "metadata": {},
   "source": [
    "### Managing Models\n",
    "\n",
    "Saved models are stored in BentoMLâ€™s model store, which is a local file directory maintained by BentoML. Users can view and manage all saved models via the `bentoml models` CLI command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882b16a",
   "metadata": {},
   "source": [
    "List\n",
    "\n",
    "```bash\n",
    "> bentoml models list\n",
    "\n",
    "Tag                        Module           Size        Creation Time        Path\n",
    "iris_clf:2uo5fkgxj27exuqj  bentoml.sklearn  5.81 KiB    2022-05-19 08:36:52  ~/bentoml/models/iris_clf/2uo5fkgxj27exuqj\n",
    "iris_clf:nb5vrfgwfgtjruqj  bentoml.sklearn  5.80 KiB    2022-05-17 21:36:27  ~/bentoml/models/iris_clf/nb5vrfgwfgtjruqj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b88d2",
   "metadata": {},
   "source": [
    "Get\n",
    "```bash\n",
    "> bentoml models get iris_clf:latest\n",
    "\n",
    "name: iris_clf\n",
    "version: 2uo5fkgxj27exuqj\n",
    "module: bentoml.sklearn\n",
    "labels: {}\n",
    "options: {}\n",
    "metadata: {}\n",
    "context:\n",
    "    framework_name: sklearn\n",
    "    framework_versions:\n",
    "      scikit-learn: 1.1.0\n",
    "    bentoml_version: 1.0.0\n",
    "    python_version: 3.8.12\n",
    "signatures:\n",
    "    predict:\n",
    "      batchable: false\n",
    "api_version: v1\n",
    "creation_time: '2022-05-19T08:36:52.456990+00:00'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa5309",
   "metadata": {},
   "source": [
    "Delete\n",
    "```bash\n",
    "> bentoml models delete iris_clf:latest -y\n",
    "\n",
    "INFO [cli] Model(tag=\"iris_clf:2uo5fkgxj27exuqj\") deleted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5f22c",
   "metadata": {},
   "source": [
    "### Model Import and Export\n",
    "Models saved with BentoML can be exported to a standalone archive file outside of the model store, for sharing models between teams or moving models between different build stages. For example:\n",
    "\n",
    "```bash\n",
    "> bentoml models export iris_clf:latest .\n",
    "\n",
    "Model(tag=\"iris_clf:2uo5fkgxj27exuqj\") exported to ./iris_clf-2uo5fkgxj27exuqj.bentomodel\n",
    "```\n",
    "\n",
    "```bash\n",
    " > bentoml models import ./iris_clf-2uo5fkgxj27exuqj.bentomodel\n",
    "\n",
    "Model(tag=\"iris_clf:2uo5fkgxj27exuqj\") imported\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab3ac9",
   "metadata": {},
   "source": [
    "### Push and Pull with Yatai\n",
    "\n",
    "**Yatai** provides a centralized Model repository that comes with flexible APIs and Web UI for managing all models (and **Bentos**) created by your team. It can be configured to store model files on cloud blob storage such as AWS S3, MinIO or GCS.\n",
    "\n",
    "Once your team have Yatai setup, you can use the bentoml models push and bentoml models pull command to get models to and from Yatai:\n",
    "\n",
    "```bash\n",
    "> bentoml models push iris_clf:latest\n",
    "\n",
    "Successfully pushed model \"iris_clf:2uo5fkgxj27exuqj\"                                       \n",
    "```\n",
    "```bash\n",
    "> bentoml models pull iris_clf:latest\n",
    "\n",
    "Successfully pulled model \"iris_clf:2uo5fkgxj27exuqj\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a58537",
   "metadata": {},
   "source": [
    "### Model Management API\n",
    "Besides the CLI commands, BentoML also provides equivalent **Python APIs** for managing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d88dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yjkim/bentoml/models/iris_clf/deocjxszyw3cllg6\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# Get\n",
    "bento_model: bentoml.Model = bentoml.models.get('iris_clf:latest')\n",
    "\n",
    "print(bento_model.path)\n",
    "print(bento_model.info.metadata)\n",
    "print(bento_model.info.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63584ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model(tag=\"iris_clf:deocjxszyw3cllg6\", path=\"/Users/yjkim/bentoml/models/iris_clf/deocjxszyw3cllg6\"), Model(tag=\"iris_clf:qfy5hzsrc6g5c6cp\", path=\"/Users/yjkim/bentoml/models/iris_clf/qfy5hzsrc6g5c6cp\")]\n"
     ]
    }
   ],
   "source": [
    "# List\n",
    "models = bentoml.models.list()\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "745ac78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yjkim/bentoml/practice/iris.bentomodel'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import/Export\n",
    "path = '/Users/yjkim/bentoml/practice/'\n",
    "\n",
    "# Export\n",
    "bentoml.models.export_model('iris_clf:latest', path + 'iris.bentomodel')\n",
    "\n",
    "# Import\n",
    "bentoml.models.import_model(path + 'iris.bentomodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push/Pull\n",
    "bentoml.models.push(\"iris_clf:latest\")\n",
    "bentoml.models.pull(\"iris_clf:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete\n",
    "bentoml.models.delete('iris_clf:latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b152ba",
   "metadata": {},
   "source": [
    "### Using Model Runner\n",
    "\n",
    "The way to run model inference in the context of a bentoml.Service, is via a Runner. The Runner abstraction gives BentoServer more flexibility in terms of how to schedule the inference computation, how to dynamically batch inference calls and better take advantage of all hardware resource available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed97213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_clf_runner = bentoml.sklearn.get(\"iris_clf:latest\").to_runner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7b8c8",
   "metadata": {},
   "source": [
    "The runner instance can then be used for creating a bentoml.Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be9c8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "svc = bentoml.Service(\"iris_classifier\", runners=[iris_clf_runner])\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def classify(input_series: np.ndarray) -> np.ndarray:\n",
    "    result = iris_clf_runner.predict.run(input_series)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5dcaa9",
   "metadata": {},
   "source": [
    "To test out the runner interface before writing the Service API callback function, you can create a local runner instance outside of a Service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8106a5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Runner.init_local' is for debugging and testing only.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Runner instance:\n",
    "iris_clf_runner = bentoml.sklearn.get('iris_clf:latest').to_runner()\n",
    "\n",
    "# Initialize the runner in current process, this is meant for development and testing only\n",
    "iris_clf_runner.init_local()\n",
    "\n",
    "# This should yield the same result as the loaded model\n",
    "iris_clf_runner.predict.run([[5.9, 3., 5.1, 1.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7536d73",
   "metadata": {},
   "source": [
    "### Model Signatures\n",
    "\n",
    "A model signature represents a method on a model object that can be called. This information is used when creating BentoML runners for this model.\n",
    "\n",
    "From the example above, the iris_clf_runner.predict.run call will pass through the function input to the modelâ€™s predict method, running from a remote runner process.\n",
    "\n",
    "For many other ML frameworks, the model objectâ€™s inference method may not be called predict. Users can customize it by specifying the model signature during save_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4344f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    " bentoml.pytorch.save_model(\n",
    "     \"demo_mnist\",   # model name in the local model store\n",
    "     trained_model,  # model instance being saved\n",
    "     signatures={    # model signatures for runner inference\n",
    "         \"classify\": {\n",
    "             \"batchable\": False,\n",
    "         }\n",
    "     }\n",
    " )\n",
    "    \n",
    "runner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\n",
    "runner.init_local()\n",
    "runner.classify.run( MODEL_INPUT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb2d18",
   "metadata": {},
   "source": [
    "A special case here is Pythonâ€™s magic method `__call__`. Similar to the Python language convention, the call to runner.run will be applied to the modelâ€™s `__call__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4022695",
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml.pytorch.save_model(\n",
    "    \"demo_mnist\",\n",
    "    trained_model,\n",
    "    signatures={\n",
    "        \"__call__\": {\n",
    "            \"batchable\": False\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "runner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\n",
    "runner.init_local()\n",
    "runner.run( MODEL_INPUT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12584c36",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "For model inference calls that supports taking a batch input, it is recommended to enable bathcing for the target model signature. In which case, `runner#run` calls made from multiple Service workers can be dynamically merged to a larger batch and run as one inference call in the runner worker. Hereâ€™s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5f2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bentoml.pytorch.save_model(\n",
    "    \"demo_mnist\",\n",
    "    trained_model,\n",
    "    signatures={\n",
    "        \"__call__\": {\n",
    "            \"batchable\": True,\n",
    "            \"batch_dim\": 0,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "runner = bentoml.pytorch.get(\"demo_mnist:latest\").to_runner()\n",
    "runner.init_local()\n",
    "runner.run( MODEL_INPUT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad2011",
   "metadata": {},
   "source": [
    "The `batch_dim` parameter determines the dimension(s) that contain multiple data when passing to this run method. The default `batch_dim`, when left unspecified, is 0.\n",
    "\n",
    "For example, if you have two inputs you want to run prediction on, `[1, 2]` and `[3, 4]`, if the array you would pass to the predict method would be `[[1, 2], [3, 4]]`, then the batch dimension would be 0. If the array you would pass to the predict method would be `[[1, 3], [2, 4]]`, then the batch dimension would be 1. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907cc51",
   "metadata": {},
   "source": [
    "For online serving workloads, adaptive batching is a critical component that contributes to the overall performance. If throughput and latency are important to you, learn more about other Runner options and batching configurations in the Using Runners and Adaptive Batching doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00021e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bentoml",
   "language": "python",
   "name": "bentoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
